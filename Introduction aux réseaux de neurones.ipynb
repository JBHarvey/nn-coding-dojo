{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux réseaux de neurones\n",
    "\n",
    "## Partie 1 - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "from dojo.dataset import Cifar10Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset\n",
    "\n",
    "Durant ce dojo, nous utiliserons le dataset CIFAR10. Ce dataset comprend 60000 images RGB 32 par 32 de 10 classes différentes:\n",
    "\n",
    "0. airplane\n",
    "1. automobile\n",
    "2. bird\n",
    "3. cat\n",
    "4. deer\n",
    "5. dog\n",
    "6. frog\n",
    "7. horse\n",
    "8. ship\n",
    "9. truck\n",
    "\n",
    "Il existe également le dataset CIFAR100, qui a lui 100 classes distinctes. Pour plus de détails, voir le [site officiel](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Évaluez les cellules suivante pour télécharger le jeu de données. Normalement, vous devriez voir qu'il y a 50000 images dans les données d'entraînement et 10000 images dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cifar10 = Cifar10Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cifar10.x_train.shape)\n",
    "print(cifar10.y_train.shape)\n",
    "print(cifar10.x_test.shape)\n",
    "print(cifar10.y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(cifar10.y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(cifar10.x_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Le problème de classification\n",
    "\n",
    "Regardons la première image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cifar10.x_train[0])\n",
    "print(cifar10.y_train[0])\n",
    "plt.imshow(cifar10.x_train[0].astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cette image est comprise par un humain comme étant une grenouille. Par contre, pour un ordinateur, cette image n'est rien de plus q'un tableau de 32 x 32 x 3 nombres à virgule flottante. Le problème de classification d'image est donc le suivant:\n",
    " \n",
    " \n",
    "> Étant donné des images $x_i$ et leurs étiquettes $y_i$, trouvez la fonction $f(X): x \\rightarrow y$ qui maximise le bon nombre d'associations entre les images et les étiquettes et qui se généralise à des images inconnues.\n",
    "\n",
    "\n",
    "### k-Nearest Neighbor\n",
    "\n",
    "Un des algorithmes les plus simple pour résoudre le problème de classification est k-Nearest Neighbor (kNN). k est un hyper-paramètre de l'algorithme, c'est-à-dire un paramètre qui est choisi par le programmeur de l'algorithme en fonction de l'application requise. L'algorithme fonctionne comme suit:\n",
    "\n",
    "Étant donné un point de test $x$, on regarde les $k$ voisins les plus proches et on procède à un votre de majorité sur la classe que devrait avoir $x$. Voici un exemple visuel où les données ont deux dimensions (x, y) et il y a trois classes possibles (bleu, rouge et vert).\n",
    "\n",
    "![kNN](http://cs231n.github.io/assets/knn.jpeg)\n",
    "\n",
    "Pour obtenir les voisins les plus proches, il faut d'abord se choisir une métrique de distance. Souvent, on utilise la distance L2 (ou distance euclidienne). Dans le cas 2D, on a:\n",
    "\n",
    "$$\n",
    "d = \\sqrt{x^2 + y^2}\n",
    "$$\n",
    "\n",
    "On peut utiliser la même métrique pour des images en comparant pixel par pixel chacune des valeurs de r, g et b.\n",
    "\n",
    "#### Exercice\n",
    "\n",
    "Complétez le fichier k_nearest_neighbor.py situé dans le dossier dojo. Dans un premier temps, complétez la fonction `compute_distances_two_loops`. Vous pouvez viusaliser le résultat avec le code suivant:\n",
    "\n",
    "> **NOTE** Nous utiliserons ici que le dixième du dataset. Les images rgb sont redimensionnés en vecteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dojo.k_nearest_neighbor import KNearestNeighbor\n",
    "\n",
    "n_of_train = 5000\n",
    "n_of_test = 500\n",
    "\n",
    "x_train, y_train = cifar10.x_train[:n_of_train], cifar10.y_train[:n_of_train]\n",
    "x_test, y_test = cifar10.x_test[:n_of_test], cifar10.y_test[:n_of_test]\n",
    "x_train = x_train.reshape((n_of_train, -1))\n",
    "x_test = x_test.reshape((n_of_test, -1))\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(x_train, y_train)\n",
    "\n",
    "dists = classifier.compute_distances_two_loops(x_test) # May take some time to compute\n",
    "print(dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(dists, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Que représentent les colonnes claires? Les colonnes foncées?\n",
    "\n",
    "Maintenant, implémentez la fonction `predict_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use k = 1 (which is Nearest Neighbor).\n",
    "y_test_pred = classifier.predict_labels(dists, k=3)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / n_of_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, n_of_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre algorithme devrait obtenir environ 27% d'accuracy. Vous pouvez jouer avec le paramètre $k$ pour regarder si les résultats s'améliorent ou non. Quelle est la meilleure valeur de k?\n",
    "\n",
    "#### Discussion\n",
    "- Peut-on utiliser kNN dans la forme présentée plus haut pour la classification de grands dataset?\n",
    "- Nommer une limitation de l'algorithme.\n",
    "- Comment avez-vous choisi la valeur de k? Avez-vous utilisé les données de test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Backpropagation\n",
    "\n",
    "Dans la partie précédente, nous avons vu une manière assez limitée de faire de la classification. En effet, il faut précalculer une matrice de distances, puis itérer sur cette matrice pour faire un vote de majorité, ce qui est extrèmement coûteux. Idéalement, nous aimerions avoir un algorithme qui apprend itérativement en s'améliorant à chaque étape. L'algorithme aurait la forme suivante:\n",
    "\n",
    "1. On donne à l'algorithme une donnée ou une mini-batch de données (ex. 64 points).\n",
    "2. L'algorithme utilise ses paramètre actuels pour prédire les étiquettes des données.\n",
    "3. L'algorithme évalue l'erreur produite sur cette évaluation\n",
    "4. L'algorithme évalue le rôle de chaque paramètre sur cette erreur.\n",
    "5. L'algorithme ajuste ces paramètres.\n",
    "6. goto 1.\n",
    "\n",
    "Les étapes 3. et 4. sont le nerf de la guerre. Ce qui est décrit en 3. est s'appelle la *loss function*. L'étape 4. est résolue par une technique qui s'appelle la *backpropagation*. Voyons ces étapes en détail à l'aide d'un exemple jouet avant de s'attaquer au problème de classification.\n",
    "\n",
    "### Fitting de courbe polynomiale\n",
    "\n",
    "Le dataset suivant est constitué de points $(x, y)$ où $y = ax^2 + bx + c + \\sigma$, où sigma représente du bruit dans la fonction.\n",
    "\n",
    "> **ATTENTION** En temps normal, on ne connais pas la distribution statistique des données d'entraînement, ce qui rend le problème extrêmement plus complexe.\n",
    "\n",
    "Notre algorithme tenteras d'estimer les paramètre a, b et c.\n",
    "Dans un premier temps, initialisons le dataset et les paramètres de la fonction à apprendre:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dojo.dataset import PolynomialDataset\n",
    "\n",
    "min_param, max_param = (0, 100)\n",
    "poly_data = PolynomialDataset(min_param=min_param, max_param=max_param)\n",
    "n_of_train = poly_data.x_train.shape[0]\n",
    "n_of_test = poly_data.x_test.shape[0]\n",
    "print(n_of_train, n_of_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Loss function*\n",
    "Écrivez la *loss function*. Il s'agit de la différence entre le $y$ prédit et le vrai $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(y_predicted, y_ground_truth):\n",
    "    return y_predicted - y_ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "Maintenant, il faut regarder comment a, b et c affectent l'erreur sur la prédiction. Pour se faire, il faut calculer les dérivées partielles de la fonction. Cela fait du sens puisque ces dérivées partiellent indiquent comment chaque paramètre influence la sortie de la fonction.\n",
    "\n",
    "$$\n",
    "f(x) = ax^2 + bx + c\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial a} = x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial b} = x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial c} = 1\n",
    "$$\n",
    "\n",
    "On vous fourni la fonction qui retourne le gradient en fonction du loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(x, loss):\n",
    "    return np.array([x**2, x, 1]) * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insérer ici des explications supplémentaires sur le gradient et la backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "a, b, c = 0.0, 0.0, 0.0\n",
    "\n",
    "print(a, b, c)\n",
    "\n",
    "for i in range(3000):\n",
    "    random_index = random.randint(0, n_of_train - 1)\n",
    "    x = poly_data.x_train[random_index]\n",
    "    y = poly_data.y_train[random_index]\n",
    "    y_predicted = a * x**2 + b * x + c\n",
    "    loss = loss_function(y_predicted, y)\n",
    "    da, db, dc = gradient(x, loss)\n",
    "    \n",
    "    a -= learning_rate * da\n",
    "    b -= learning_rate * db\n",
    "    c -= learning_rate * dc\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(\"Step %d: a: %f b: %f c: %f\" % (i, round(a, 2), round(b, 2), round(c, 2)))\n",
    "        print(\"---loss: %f\" % loss)\n",
    "        \n",
    "print(\"TRAINING FINISHED\")\n",
    "print(\"ESTIMATED PARAMS a: %f b: %f c: %f\" % (round(a, 2), round(b, 2), round(c, 2)))\n",
    "print(\"ACTUAL PARAMS a: %d b: %d c: %d\" % (poly_data.a, poly_data.b, poly_data.c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "- Comment se comporte le loss au fil du temps?\n",
    "- Décrivez le comportement des paramètres a, b, c au fil du temps.\n",
    "- À quoi sert le learning rate? Modifiez la valeur et observez le comportement.\n",
    "- Modifier les valeurs initiales des paramètres et observez le comportement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Classification linéaire\n",
    "\n",
    "Dans cette partie, nous développerons un modèle de classification linéaire pour le dataset Cifar10. Le but est d'avoir une approche itérative de descente de gradient comme à la partie 2. Si vous comprenez bien ce modèle, vous comprendrez aussi les réseaux de neurones, qui sont simplement la combinaison de plusieurs modèles linéaires.\n",
    "\n",
    "\n",
    "Revenons au problème de classification. Rappel:\n",
    "\n",
    "> Étant donné des images $x_i$ et leurs étiquettes $y_i$, trouvez la fonction $f(X): x \\rightarrow y$ qui maximise le bon nombre d'associations entre les images et les étiquettes et qui se généralise à des images inconnues.\n",
    "\n",
    "\n",
    "On peut décomposer la fonction $f$ en plusieurs fonctions. Au lieu d'avoir une fonction unique qui retourne directment l'étiquette $y$ de l'image, on peut avoir une fonction par classe du dataset et chaque fonction retourne le score associé à l'image en entrée qui indique à quel point il est probable que l'image est cette classe. Par exemple, si le dataset a seulement deux classes, 0 pour chien et 1 pour chat, nous aurions deux fonctions $f_0(X)$ et $f_1(X)$. Pour une image donnée $x_i$, si on a $f_0(x_i) = 0.6$ et $f_1(x_i) = 0.2$, on en déduit que l'image $x_i$  est un chien, car selon les fonctions, il est plus probable que l'image soit un chien. Voici une représentation visuelle des fonction $f_j(x)$. À noter, la figure est en 2D, mais dans le cas réel, les fonctions séparent les images dans un espace à $32 \\times 32 \\times 3 = 3072$ dimensions.\n",
    "\n",
    "![classification figure](http://cs231n.github.io/assets/pixelspace.jpeg)\n",
    "\n",
    "La classification linéaire est un modèle très simple pour représenter les fonctions $f_j(x)$. La formulation est la suivante:\n",
    "\n",
    "$$\n",
    "f_j(x) = w_j \\cdot x + b_j\n",
    "$$\n",
    "\n",
    "> **NOTE** Pour simplifier les calculs, $x$ est redimensionné en un vecteur de 3072 éléments. Ainsi, il est très facile d'associer un point à chaque pixel et multiplier ces points à $x$, puisque cela se résume en un produit scalaire.\n",
    "\n",
    "Dans la formule précédente, le vecteur $w_j$ représente les poids (ou *weights* en anglais) à associer à chaque pixel de l'image $x$ et $b_j$ est un scalaire qui représente le biais, c'est-à-dire si à priori il est plus probable d'observer la classe $j$.\n",
    "\n",
    "\n",
    "Pour que le calcul soit plus efficace, il est possible de calculer tous les scores $f_j(x)$ en même temps. Il suffit de mettre tous les vecteurs $w_j$ dans une matrice $W$ où chaque ligne $j$ est $w_j$ et de faire le produit matriciel $Wx$, puis additionner un vecteur $b$ qui contient les biais. Cela revient exactement au même que de faire les produit scalaires séparemment. Voici une représentation visuelle cette opération:\n",
    "\n",
    "![weight matrix](http://cs231n.github.io/assets/imagemap.jpg)\n",
    "\n",
    "\n",
    "### Généralisation\n",
    "\n",
    "La généralisation est un élément primordial en machine learning. C'est bien beau d'être capable de classifier correctement les données d'entraînement, mais il faut à tout pris éviter le phénomène d'*overfitting*. L'exemple classique est de trouver la courbe qui passe par une série de points. Si on a $n$ points, il est possible de trouver un polynôme de degré $n + 1$ qui passe parfaitement par tous les points. Par contre, cette courbe risque de moins bien performer qu'un modèle plus simple. pour passer à travers les autres points de la distribution que l'on n'as pas encore observé.\n",
    "\n",
    "![overfitting](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Overfitted_Data.png/300px-Overfitted_Data.png)\n",
    "\n",
    "\n",
    "Généralement, en machine learning, on sépare les données en deux groupes: *training* et *testing*. Ainsi, on peut évaluer la performance du modèle sur les données inconnues en utilisant des données qui n'ont pas servi à l'entraînement.\n",
    "\n",
    "\n",
    "### Exercice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
